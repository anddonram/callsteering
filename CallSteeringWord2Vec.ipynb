{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Steering\n",
    "\n",
    "En este cuaderno abordaremos el problema de enrutamiento de llamadas como un problema de clasficación. Para mayor flexibilidad de uso, los métodos principales se encuentran en un fichero Python corriente, por lo que puede importarse en otras aplicaciones o cuadernos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, importaremos todos las funciones y objetos auxiliares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from call import *\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, obtenemos los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text,target,classes,classes_reverse=recover_from_files(design_filepath)\n",
    "\n",
    "# Convert class to numeric\n",
    "y_data=[classes_reverse[x] for x in target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos un par de funciones auxiliares. \n",
    "\n",
    "La primera construye una tubería que recibe los datos y realiza automáticamente el procesamiento. En concreto, se utilizará un modelo basado en \"bag of words\", por lo que se pierde la información sobre el orden de la sentencia. El uso de tuberías permite la búsqueda de hiperparámetros para el preprocesado. Por ejemplo, si es mejor utilizar stop words o no.\n",
    "\n",
    "La segunda función compara dos clasificadores y devuelve el mejor respecto a la precisión media conseguida por cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al principio, no tenemos mejor clasificador\n",
    "best=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipe(clf,params):\n",
    "    \"\"\"\n",
    "    Dado un clasificador y sus parámetros, construye una tubería que procesa los datos\n",
    "    \"\"\"\n",
    "    prefix=\"clf__\"\n",
    "    parameters={'counter__stop_words':[None,'english'],\n",
    "           'tf_idf__use_idf':[True,False]}\n",
    "    if type(params) is dict:\n",
    "        for k in params:\n",
    "            #Add hyperparameters to dict\n",
    "            parameters[prefix+k]=params[k]\n",
    "    elif type(params) is list:\n",
    "        par=parameters\n",
    "        parameters=[]\n",
    "        for line in params:\n",
    "            aux_params={}\n",
    "            aux_params.update(par)\n",
    "            for k in line:\n",
    "                #Add hyperparameters to dict\n",
    "                aux_params[prefix+k]=line[k]\n",
    "            parameters.append(aux_params)\n",
    "    #Create pipeline\n",
    "    count_vect = StemmedCountVectorizer()\n",
    "\n",
    "    tf_transformer = TfidfTransformer()\n",
    "    #svd=TruncatedSVD(n_components=300)\n",
    "\n",
    "    pipe=Pipeline([('counter',count_vect),\n",
    "        ('tf_idf',tf_transformer),\n",
    "        ('clf',clf)\n",
    "    #    ('svd',svd)\n",
    "        ])\n",
    "    \n",
    "    return pipe, parameters\n",
    "\n",
    "\n",
    "def compare_best_classifier(clf1,clf2):\n",
    "    \"\"\"\n",
    "    Dados dos clasificadores entrenados con GridSearch, devuelve el que tiene la mejor tasa de aciertos media\n",
    "    \"\"\"\n",
    "    val_metric='mean_test_acc'\n",
    "    if clf1 is None:\n",
    "        return clf2\n",
    "    return clf1 if clf1.cv_results_[val_metric][clf1.best_index_]>clf2.cv_results_[val_metric][clf2.best_index_] else clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores\n",
    "\n",
    "Ya podemos entrenar los clasificadores. Comenzaremos por Naive Bayes, pues suele utilizarse como clasificador base para comparar el rendimiento, dada su simplicidad a la par que efectividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...lse,\n",
      "         use_idf=False)), ('clf', MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))])\n",
      "Train acc 0.9756927059465916\n",
      "Train prec 0.9759217720903675\n",
      "Train rec 0.9756927059465916\n",
      "Train f1 0.9755286963157636\n",
      "Test acc 0.796887159533074\n",
      "Test prec 0.8080580587879579\n",
      "Test rec 0.796887159533074\n",
      "Test f1 0.7918626322691907\n",
      "{'clf__alpha': 0.01, 'counter__stop_words': None, 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "parameters =  {'alpha': [0.01, 1, 5]}\n",
    "clf=MultinomialNB()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas de validación medias rondan el 79%, lo que no está mal. Curiosamente, la mejor combinación de parámetros es no usar stop words ni idf para tener en cuenta la rareza de los términos en los documentos.\n",
    "\n",
    "El siguiente clasificador que vamos a utilizar es regresión logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Train acc 0.9869732892719216\n",
      "Train prec 0.9871680304503782\n",
      "Train rec 0.9869732892719216\n",
      "Train f1 0.9869425132763509\n",
      "Test acc 0.8809338521400778\n",
      "Test prec 0.8868755181358511\n",
      "Test rec 0.8809338521400778\n",
      "Test f1 0.879476739178115\n",
      "{'clf__C': 10, 'clf__penalty': 'l2', 'counter__stop_words': None, 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "parameters =  {'penalty':('l1','l2'),'C': [0.1, 1, 10]}\n",
    "clf = LogisticRegression()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, el rendimiento conseguido es sensiblemente superior al de Naive Bayes. De nuevo, no se utilizan stop words Nos quedamos con este como el mejor hasta ahora.\n",
    "\n",
    "Probaremos ahora con máquinas de vectores soporte (SVM). Para estos hay que ajustar el kernel y sus parámetros, por lo que probaremos con varios de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Train acc 0.9768577307534259\n",
      "Train prec 0.9777753732532293\n",
      "Train rec 0.9768577307534259\n",
      "Train f1 0.9768362095903804\n",
      "Test acc 0.8856031128404669\n",
      "Test prec 0.8950015376546079\n",
      "Test rec 0.8856031128404669\n",
      "Test f1 0.8846482078694289\n",
      "{'clf__C': 1, 'clf__kernel': 'linear', 'counter__stop_words': None, 'tf_idf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "  {'C': [0.1, 1, 10], 'kernel': ['linear']},\n",
    "  {'C': [0.1, 1, 10], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "clf = SVC()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que conseguimos algo más de rendimiento con este clasificador. También mejoramos otras métricas como la precisión y el f1.\n",
    "\n",
    "Como el mejor kernel ha resultado ser 'linear', podemos probar con una implementación más eficiente para explorar más parámetros y probar a obtener un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0))])\n",
      "Train acc 0.9871595358581313\n",
      "Train prec 0.9875053300884554\n",
      "Train rec 0.9871595358581313\n",
      "Train f1 0.9871593112778706\n",
      "Test acc 0.8863813229571984\n",
      "Test prec 0.8951286926001604\n",
      "Test rec 0.8863813229571984\n",
      "Test f1 0.8847856310713369\n",
      "{'clf__C': 1, 'clf__class_weight': 'balanced', 'clf__tol': 0.001, 'counter__stop_words': None, 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'C': [0.1, 1, 10],'tol':[1e-3,1e-4,1e-5],'class_weight':[None,'balanced']}\n",
    " \n",
    "clf = LinearSVC()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejora ha sido muy pequeña, pero suficiente como para quedarnos con esta.\n",
    "\n",
    "\n",
    "El próximo clasificador utiliza descenso por gradiente estocástico. Por lo general, esto le permite escapar de mínimos locales para conseguir mejores resultados. Entrenaremos tanto un SVM como un regresor logístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...et', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=0.0001, verbose=0, warm_start=False))])\n",
      "Train acc 0.9873575043573032\n",
      "Train prec 0.9875341071831795\n",
      "Train rec 0.9873575043573032\n",
      "Train f1 0.9873440637394835\n",
      "Test acc 0.8832684824902723\n",
      "Test prec 0.8913748356083889\n",
      "Test rec 0.8832684824902723\n",
      "Test f1 0.8813688940664878\n",
      "{'clf__alpha': 0.0001, 'clf__loss': 'log', 'clf__max_iter': 1000, 'clf__penalty': 'elasticnet', 'clf__tol': 0.0001, 'counter__stop_words': None, 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "parameters={'max_iter': [1000] ,'tol':[1e-2,1e-3,1e-4],'penalty':['l2','l1','elasticnet'],\n",
    "    'loss':('hinge','log'), 'alpha':[0.0001,0.001]}\n",
    "clf=SGDClassifier()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También consigue un rendimiento alto, aunque se queda ligeramente por debajo del clasificador anterior.\n",
    "\n",
    "Por último, vamos a probar con árboles de decisión. Utilizaremos un árbol estándar y luego la variante random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "Train acc 0.9929953308403136\n",
      "Train prec 0.9932892721304407\n",
      "Train rec 0.9929953308403136\n",
      "Train f1 0.9930141515998896\n",
      "Test acc 0.7844357976653696\n",
      "Test prec 0.8069069528120919\n",
      "Test rec 0.7844357976653696\n",
      "Test f1 0.7843418419610835\n",
      "{'clf__criterion': 'gini', 'clf__min_samples_split': 0.0001, 'counter__stop_words': 'english', 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "parameters={'min_samples_split':[0.0001,0.001,0.01],'criterion':('gini','entropy')}\n",
    "clf=DecisionTreeClassifier()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('counter', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), preprocessor=None, stop_wor...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "Train acc 0.9883192993734117\n",
      "Train prec 0.9885866218875702\n",
      "Train rec 0.9883192993734117\n",
      "Train f1 0.9883146241349395\n",
      "Test acc 0.8171206225680934\n",
      "Test prec 0.8330632928828049\n",
      "Test rec 0.8171206225680934\n",
      "Test f1 0.816567594597479\n",
      "{'clf__criterion': 'gini', 'clf__min_samples_split': 0.0001, 'clf__n_estimators': 10, 'counter__stop_words': 'english', 'tf_idf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "parameters={'n_estimators':[5,10], 'criterion':('gini','entropy'),'min_samples_split':[0.0001,0.001,0.01]}\n",
    "clf= RandomForestClassifier()\n",
    "pipe,params=construct_pipe(clf,parameters)\n",
    "estimator=busqueda_cv(pipe,text,y_data,params)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(estimator.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por desgracia, los árboles de decisión no clasifican tan bien en este caso como los SVM o la regresión logística. El mejor de todos ha sido el LinearSVC. Con este último haremos la evaluación final con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendimiento_test(clf):\n",
    "    \"\"\"\n",
    "    Dado un clasificador, obtiene su rendimiento con un conjunto de prueba\n",
    "    \"\"\"\n",
    "    text,target,classes,classes_reverse=recover_from_files(eval_filepath)\n",
    "\n",
    "    # Convert class to numeric\n",
    "    y_test=[classes_reverse[x] for x in target]\n",
    "\n",
    "    #Vectorize data\n",
    "\n",
    "    print(\"Final test\")\n",
    "    for key in scoring:\n",
    "        print(key,scoring[key](clf,text,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test\n",
      "acc 0.8807947019867549\n",
      "prec 0.8887261928652658\n",
      "rec 0.8807947019867549\n",
      "f1 0.881159789437935\n",
      "{'clf__C': 1, 'clf__class_weight': 'balanced', 'clf__tol': 0.001, 'counter__stop_words': None, 'tf_idf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "rendimiento_test(best)\n",
    "print(best.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento final es de un 88%. Para las cuatro métricas se consigue un valor similar, y además no se aleja de los datos de validación del clasificador. Por tanto, podemos concluir que el clasificador es considerablemente bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "glove_path=\"../glove.6B/glove.6B.50d.txt\"\n",
    "w2v={}\n",
    "with open(glove_path, encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        splits=line.strip().split(\" \")\n",
    "        w2v[splits[0]]=splits[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04502558 0.03595504 0.03949976 0.04502558 0.04502558 0.04502558\n",
      " 0.04258036 0.04502558 0.04502558 0.04502558 0.04258036 0.04502558\n",
      " 0.04084546 0.03666534 0.04502558 0.02869432 0.03949976 0.01382718\n",
      " 0.04502558 0.02638463 0.04502558 0.04084546 0.03666534 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.03840025 0.04502558 0.04084546\n",
      " 0.0271121  0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.03595504 0.04502558 0.03747062 0.03949976\n",
      " 0.03949976 0.04502558 0.03113953 0.03949976 0.02611656 0.04258036\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04084546 0.03666534\n",
      " 0.04258036 0.04502558 0.03747062 0.01902873 0.04258036 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.02493027 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04258036 0.04084546 0.03144886 0.04502558 0.0254943  0.03840025\n",
      " 0.0156945  0.04502558 0.04084546 0.03329051 0.04258036 0.04502558\n",
      " 0.03949976 0.04502558 0.04502558 0.02811954 0.04084546 0.03666534\n",
      " 0.04258036 0.04502558 0.04502558 0.04502558 0.03840025 0.03840025\n",
      " 0.02561372 0.03474487 0.04258036 0.04502558 0.04502558 0.04502558\n",
      " 0.04258036 0.04084546 0.04502558 0.03531965 0.04502558 0.04502558\n",
      " 0.03747062 0.04258036 0.04502558 0.03329051 0.04502558 0.04502558\n",
      " 0.04502558 0.03840025 0.04502558 0.04502558 0.04258036 0.03474487\n",
      " 0.03666534 0.04084546 0.04502558 0.03531965 0.04502558 0.03840025\n",
      " 0.01936154 0.04258036 0.03177492 0.03949976 0.04258036 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.04258036 0.04258036 0.04084546\n",
      " 0.03747062 0.04258036 0.03840025 0.04502558 0.04258036 0.04502558\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.04502558 0.04502558\n",
      " 0.03747062 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04258036 0.03329051 0.03595504 0.04084546 0.04502558 0.03422013\n",
      " 0.04502558 0.02503894 0.04258036 0.04502558 0.04502558 0.04258036\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.04258036 0.04502558\n",
      " 0.04502558 0.03373742 0.04502558 0.04258036 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.02503894 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.03666534 0.04502558 0.04502558\n",
      " 0.04084546 0.04258036 0.04258036 0.03949976 0.04502558 0.04258036\n",
      " 0.04502558 0.04258036 0.04258036 0.04502558 0.04502558 0.03373742\n",
      " 0.04502558 0.03029668 0.04258036 0.04502558 0.04502558 0.04084546\n",
      " 0.04258036 0.04502558 0.04502558 0.03840025 0.04502558 0.04502558\n",
      " 0.04258036 0.03840025 0.04502558 0.04084546 0.04502558 0.02911039\n",
      " 0.03422013 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558 0.04258036\n",
      " 0.04502558 0.04502558 0.02776469 0.02598687 0.04084546 0.0268105\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.02955731 0.02889877\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.04502558 0.04258036\n",
      " 0.03747062 0.04502558 0.04084546 0.02742957 0.04502558 0.04258036\n",
      " 0.04258036 0.04502558 0.03949976 0.03747062 0.03248523 0.04502558\n",
      " 0.04502558 0.03287444 0.03177492 0.04502558 0.04084546 0.04502558\n",
      " 0.04084546 0.03949976 0.0245142  0.04502558 0.04502558 0.04258036\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558 0.03747062\n",
      " 0.04258036 0.04502558 0.03747062 0.0210822  0.02263039 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558 0.03422013\n",
      " 0.03531965 0.02638463 0.02285516 0.04084546 0.03747062 0.04258036\n",
      " 0.03373742 0.04258036 0.02759481 0.04502558 0.03840025 0.04502558\n",
      " 0.03949976 0.04502558 0.04502558 0.03287444 0.04084546 0.02793951\n",
      " 0.04258036 0.04084546 0.04502558 0.04502558 0.03747062 0.04258036\n",
      " 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04258036 0.04258036 0.04502558 0.04502558 0.0213736  0.03840025\n",
      " 0.03029668 0.04502558 0.04502558 0.03949976 0.04502558 0.04502558\n",
      " 0.03474487 0.03531965 0.02358458 0.01859918 0.02316851 0.04258036\n",
      " 0.04502558 0.04502558 0.03474487 0.04502558 0.04502558 0.03949976\n",
      " 0.02008791 0.04502558 0.03840025 0.03373742 0.02125534 0.03474487\n",
      " 0.04502558 0.04502558 0.04502558 0.03474487 0.04502558 0.04502558\n",
      " 0.04258036 0.04502558 0.04258036 0.04502558 0.02270438 0.03666534\n",
      " 0.04084546 0.04502558 0.04502558 0.04502558 0.04258036 0.03474487\n",
      " 0.04502558 0.04502558 0.04084546 0.04502558 0.04084546 0.04258036\n",
      " 0.04258036 0.04258036 0.03840025 0.04084546 0.04502558 0.03004002\n",
      " 0.04084546 0.04502558 0.04084546 0.04258036 0.04502558 0.04502558\n",
      " 0.04502558 0.04084546 0.02849658 0.04084546 0.04258036 0.04258036\n",
      " 0.04258036 0.03747062 0.04502558 0.04502558 0.04502558 0.04258036\n",
      " 0.03113953 0.03666534 0.04258036 0.03840025 0.03747062 0.03949976\n",
      " 0.04502558 0.04502558 0.04502558 0.03949976 0.04258036 0.04502558\n",
      " 0.03840025 0.04502558 0.04502558 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.0308453  0.04502558 0.04502558 0.03747062 0.04258036\n",
      " 0.03595504 0.04084546 0.04502558 0.04258036 0.04084546 0.04502558\n",
      " 0.04258036 0.04258036 0.03949976 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.03373742 0.04502558\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.04502558 0.0271121\n",
      " 0.04502558 0.04502558 0.04084546 0.03474487 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04258036 0.04502558 0.03747062 0.04258036\n",
      " 0.04084546 0.04502558 0.03474487 0.03949976 0.04502558 0.03949976\n",
      " 0.04084546 0.03177492 0.04502558 0.04502558 0.04084546 0.03949976\n",
      " 0.04258036 0.04084546 0.03840025 0.02849658 0.04502558 0.04258036\n",
      " 0.04502558 0.04258036 0.03144886 0.04084546 0.04502558 0.04502558\n",
      " 0.04502558 0.04502558 0.04502558 0.04258036 0.04502558 0.04258036\n",
      " 0.04258036 0.04502558 0.04502558 0.04502558 0.02695941 0.04502558\n",
      " 0.04502558 0.04258036 0.02726875 0.04502558 0.04502558 0.04084546\n",
      " 0.04502558 0.04502558 0.04502558 0.03177492 0.03949976 0.04258036\n",
      " 0.02573555 0.04502558 0.03840025 0.04502558 0.04502558 0.04502558\n",
      " 0.04502558 0.03949976 0.04502558 0.04502558 0.03840025 0.03840025\n",
      " 0.04502558 0.01819821 0.04502558 0.02955731 0.03949976 0.03747062\n",
      " 0.04502558 0.03747062 0.04502558 0.03949976 0.04502558 0.04502558\n",
      " 0.04502558 0.03840025 0.04502558 0.04502558 0.04084546 0.04502558\n",
      " 0.03287444 0.03248523 0.03840025 0.04502558 0.04502558 0.04258036\n",
      " 0.04084546 0.02869432 0.04502558 0.04502558 0.04502558 0.03949976\n",
      " 0.04258036 0.04502558 0.04258036 0.01809332 0.04502558 0.04502558\n",
      " 0.03949976 0.03949976 0.02932971 0.04258036 0.03949976 0.04502558\n",
      " 0.04258036 0.04502558 0.04258036 0.04084546 0.03840025 0.04502558\n",
      " 0.04502558 0.03949976 0.04258036 0.04084546 0.02911039 0.03474487\n",
      " 0.04502558 0.04258036 0.03840025 0.04258036 0.03595504 0.04258036\n",
      " 0.04502558 0.03531965]\n"
     ]
    }
   ],
   "source": [
    "#Create pipeline\n",
    "count_vect = StemmedCountVectorizer()\n",
    "\n",
    "tf_transformer = TfidfTransformer()\n",
    "pipe=Pipeline([('counter',count_vect),\n",
    "    ('tf_idf',tf_transformer)  ])\n",
    "text_vectorized=pipe.fit_transform(text)\n",
    "unit_vector=pipe.steps[1][1].transform(np.ones((text_vectorized.shape[1]))).toarray()[0]\n",
    "print(unit_vector)\n",
    "def idf(word):\n",
    "    # get vocabulary index\n",
    "    idx=pipe.steps[0][1].vocabulary_.get(word,-1)\n",
    "    res=min(unit_vector)\n",
    "    if idx>-1:\n",
    "        res=unit_vector[idx]\n",
    "#     else:\n",
    "#         print(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def word2vec(text):\n",
    "    dim=[0]*len(w2v[\",\"])\n",
    "    return np.array([w2v.get(stemmer.stem(word.lower()),dim) for word in nltk.word_tokenize(text)],dtype=float)\n",
    "\n",
    "def idf_vec(text):\n",
    "    w2vec=word2vec(text)\n",
    "    \n",
    "    for i,word in enumerate(nltk.word_tokenize(text)):\n",
    "        w2vec[:,i]=w2vec[:,i]*idf(word.lower()) \n",
    "    return w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2817e-01  1.5858e-01 -3.8843e-01 -3.9108e-01  6.8366e-01  8.1259e-04\n",
      "  -2.2981e-01 -6.3358e-01 -2.7663e-01  4.0934e-01 -6.5128e-01  8.4610e-01\n",
      "  -9.9040e-01  2.0696e-01  1.2567e+00  6.4774e-02  6.5813e-01  3.9954e-01\n",
      "   7.6104e-02 -5.4083e-01 -3.2438e-01  8.4560e-01  1.7273e-01 -1.3504e-01\n",
      "   3.9626e-01 -2.3358e+00 -1.6576e+00  5.9957e-01  1.0876e+00 -1.0118e+00\n",
      "   3.3300e+00  7.5853e-02 -6.5637e-01 -1.5799e-02 -8.5429e-01 -4.7358e-01\n",
      "   8.2404e-02 -6.9719e-01  4.6647e-01 -3.2044e-01 -4.5517e-01  3.0804e-01\n",
      "   7.5020e-02 -2.1783e-02  1.0823e-01 -3.3060e-02 -2.5140e-01  8.8184e-02\n",
      "  -2.2215e-01  1.4971e+00]\n",
      " [-3.8497e-01  8.0092e-01  6.4106e-02 -2.8355e-01 -2.6759e-02 -3.4532e-01\n",
      "  -6.4253e-01 -1.1729e-01 -3.3257e-01  5.5243e-01 -8.7813e-02  9.0350e-01\n",
      "   4.7102e-01  5.6657e-01  6.9850e-01 -3.5229e-01 -8.6542e-01  9.0573e-01\n",
      "   3.5760e-02 -7.1705e-02 -1.2327e-01  5.4923e-01  4.7005e-01  3.5572e-01\n",
      "   1.2611e+00 -6.7581e-01 -9.4983e-01  6.8666e-01  3.8710e-01 -1.3492e+00\n",
      "   6.3512e-01  4.6416e-01 -4.8814e-01  8.3827e-01 -9.2460e-01 -3.3722e-01\n",
      "   5.3741e-01 -1.0616e+00 -8.1403e-02 -6.7111e-01  3.0923e-01 -3.9230e-01\n",
      "  -5.5002e-01 -6.8827e-01  5.8049e-01 -1.1626e-01  1.3139e-02 -5.7654e-01\n",
      "   4.8833e-02  6.7204e-01]\n",
      " [ 1.1891e-01  1.5255e-01 -8.2073e-02 -7.4144e-01  7.5917e-01 -4.8328e-01\n",
      "  -3.1009e-01  5.1476e-01 -9.8708e-01  6.1757e-04 -1.5043e-01  8.3770e-01\n",
      "  -1.0797e+00 -5.1460e-01  1.3188e+00  6.2007e-01  1.3779e-01  4.7108e-01\n",
      "  -7.2874e-02 -7.2675e-01 -7.4116e-01  7.5263e-01  8.8180e-01  2.9561e-01\n",
      "   1.3548e+00 -2.5701e+00 -1.3523e+00  4.5880e-01  1.0068e+00 -1.1856e+00\n",
      "   3.4737e+00  7.7898e-01 -7.2929e-01  2.5102e-01 -2.6156e-01 -3.4684e-01\n",
      "   5.5841e-01  7.5098e-01  4.9830e-01 -2.6823e-01 -2.7443e-03 -1.8298e-02\n",
      "  -2.8096e-01  5.5318e-01  3.7706e-02  1.8555e-01 -1.5025e-01 -5.7512e-01\n",
      "  -2.6671e-01  9.2121e-01]\n",
      " [ 3.4664e-01  3.9805e-01  4.8970e-01 -5.1421e-01  5.4574e-01 -1.2005e+00\n",
      "   3.2107e-01  7.4004e-01 -1.4979e+00 -1.9651e-01 -1.2631e-01 -3.7703e-01\n",
      "  -6.2569e-01  3.8792e-02  1.0579e+00  7.7199e-01 -1.8589e-01  1.3032e+00\n",
      "  -7.2128e-01  4.0231e-01  6.6442e-02  1.2315e+00  9.3956e-01  1.3903e+00\n",
      "   1.5334e+00 -1.4730e+00 -3.4997e-01  3.1562e-01  9.0691e-01  4.5498e-01\n",
      "   2.5481e+00  1.6410e-01 -6.0700e-01  2.7061e-01 -7.9072e-01 -1.1460e+00\n",
      "   9.1795e-01 -1.1797e-01  2.3526e-01 -1.2659e-01  6.6527e-01 -9.1816e-01\n",
      "   1.0048e-01  7.0457e-01 -2.1777e-01  5.2479e-01 -5.4452e-01  8.6576e-02\n",
      "   3.4037e-01  1.3588e+00]\n",
      " [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00  0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x=word2vec(\"\\\"hello I am yopoe\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.77222941e-03  4.40292509e-03 -5.37089077e-03 -5.40753279e-03\n",
      "   9.45308855e-03  8.12590000e-04 -2.29810000e-01 -6.33580000e-01\n",
      "  -2.76630000e-01  4.09340000e-01 -6.51280000e-01  8.46100000e-01\n",
      "  -9.90400000e-01  2.06960000e-01  1.25670000e+00  6.47740000e-02\n",
      "   6.58130000e-01  3.99540000e-01  7.61040000e-02 -5.40830000e-01\n",
      "  -3.24380000e-01  8.45600000e-01  1.72730000e-01 -1.35040000e-01\n",
      "   3.96260000e-01 -2.33580000e+00 -1.65760000e+00  5.99570000e-01\n",
      "   1.08760000e+00 -1.01180000e+00  3.33000000e+00  7.58530000e-02\n",
      "  -6.56370000e-01 -1.57990000e-02 -8.54290000e-01 -4.73580000e-01\n",
      "   8.24040000e-02 -6.97190000e-01  4.66470000e-01 -3.20440000e-01\n",
      "  -4.55170000e-01  3.08040000e-01  7.50200000e-02 -2.17830000e-02\n",
      "   1.08230000e-01 -3.30600000e-02 -2.51400000e-01  8.81840000e-02\n",
      "  -2.22150000e-01  1.49710000e+00]\n",
      " [-5.32304873e-03  2.22372983e-02  8.86405076e-04 -3.92069634e-03\n",
      "  -3.70001457e-04 -3.45320000e-01 -6.42530000e-01 -1.17290000e-01\n",
      "  -3.32570000e-01  5.52430000e-01 -8.78130000e-02  9.03500000e-01\n",
      "   4.71020000e-01  5.66570000e-01  6.98500000e-01 -3.52290000e-01\n",
      "  -8.65420000e-01  9.05730000e-01  3.57600000e-02 -7.17050000e-02\n",
      "  -1.23270000e-01  5.49230000e-01  4.70050000e-01  3.55720000e-01\n",
      "   1.26110000e+00 -6.75810000e-01 -9.49830000e-01  6.86660000e-01\n",
      "   3.87100000e-01 -1.34920000e+00  6.35120000e-01  4.64160000e-01\n",
      "  -4.88140000e-01  8.38270000e-01 -9.24600000e-01 -3.37220000e-01\n",
      "   5.37410000e-01 -1.06160000e+00 -8.14030000e-02 -6.71110000e-01\n",
      "   3.09230000e-01 -3.92300000e-01 -5.50020000e-01 -6.88270000e-01\n",
      "   5.80490000e-01 -1.16260000e-01  1.31390000e-02 -5.76540000e-01\n",
      "   4.88330000e-02  6.72040000e-01]\n",
      " [ 1.64418974e-03  4.23550398e-03 -1.13483798e-03 -1.02520229e-02\n",
      "   1.04971788e-02 -4.83280000e-01 -3.10090000e-01  5.14760000e-01\n",
      "  -9.87080000e-01  6.17570000e-04 -1.50430000e-01  8.37700000e-01\n",
      "  -1.07970000e+00 -5.14600000e-01  1.31880000e+00  6.20070000e-01\n",
      "   1.37790000e-01  4.71080000e-01 -7.28740000e-02 -7.26750000e-01\n",
      "  -7.41160000e-01  7.52630000e-01  8.81800000e-01  2.95610000e-01\n",
      "   1.35480000e+00 -2.57010000e+00 -1.35230000e+00  4.58800000e-01\n",
      "   1.00680000e+00 -1.18560000e+00  3.47370000e+00  7.78980000e-01\n",
      "  -7.29290000e-01  2.51020000e-01 -2.61560000e-01 -3.46840000e-01\n",
      "   5.58410000e-01  7.50980000e-01  4.98300000e-01 -2.68230000e-01\n",
      "  -2.74430000e-03 -1.82980000e-02 -2.80960000e-01  5.53180000e-01\n",
      "   3.77060000e-02  1.85550000e-01 -1.50250000e-01 -5.75120000e-01\n",
      "  -2.66710000e-01  9.21210000e-01]\n",
      " [ 4.79305300e-03  1.10517362e-02  6.77116909e-03 -7.11007322e-03\n",
      "   7.54604415e-03 -1.20050000e+00  3.21070000e-01  7.40040000e-01\n",
      "  -1.49790000e+00 -1.96510000e-01 -1.26310000e-01 -3.77030000e-01\n",
      "  -6.25690000e-01  3.87920000e-02  1.05790000e+00  7.71990000e-01\n",
      "  -1.85890000e-01  1.30320000e+00 -7.21280000e-01  4.02310000e-01\n",
      "   6.64420000e-02  1.23150000e+00  9.39560000e-01  1.39030000e+00\n",
      "   1.53340000e+00 -1.47300000e+00 -3.49970000e-01  3.15620000e-01\n",
      "   9.06910000e-01  4.54980000e-01  2.54810000e+00  1.64100000e-01\n",
      "  -6.07000000e-01  2.70610000e-01 -7.90720000e-01 -1.14600000e+00\n",
      "   9.17950000e-01 -1.17970000e-01  2.35260000e-01 -1.26590000e-01\n",
      "   6.65270000e-01 -9.18160000e-01  1.00480000e-01  7.04570000e-01\n",
      "  -2.17770000e-01  5.24790000e-01 -5.44520000e-01  8.65760000e-02\n",
      "   3.40370000e-01  1.35880000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "x=idf_vec(\"\\\"hello I am yopoe\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.77284684e-04,  8.38549271e-03,  2.30369083e-04, -5.33806505e-03,\n",
       "        5.42526200e-03, -4.05657482e-01, -1.72272000e-01,  1.00786000e-01,\n",
       "       -6.18836000e-01,  1.53175514e-01, -2.03166600e-01,  4.42054000e-01,\n",
       "       -4.44954000e-01,  5.95444000e-02,  8.66380000e-01,  2.20908800e-01,\n",
       "       -5.10780000e-02,  6.15910000e-01, -1.36458000e-01, -1.87395000e-01,\n",
       "       -2.24473600e-01,  6.75792000e-01,  4.92828000e-01,  3.81318000e-01,\n",
       "        9.09112000e-01, -1.41094200e+00, -8.61940000e-01,  4.12130000e-01,\n",
       "        6.77682000e-01, -6.18324000e-01,  1.99738400e+00,  2.96618600e-01,\n",
       "       -4.96160000e-01,  2.68820200e-01, -5.66234000e-01, -4.60728000e-01,\n",
       "        4.19234800e-01, -2.25156000e-01,  2.23725400e-01, -2.77274000e-01,\n",
       "        1.03317140e-01, -2.04143600e-01, -1.31096000e-01,  1.09539400e-01,\n",
       "        1.01731200e-01,  1.12204000e-01, -1.86606200e-01, -1.95380000e-01,\n",
       "       -1.99314000e-02,  8.89830000e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[idf_vec(line) for line in text]\n",
    "for mat,line in zip(x,text):\n",
    "    for i,word in enumerate(mat):\n",
    "        if (mat==0).all():\n",
    "            print(mat)\n",
    "            print(line.split()[i])\n",
    "            \n",
    "x_transformed=[np.mean(line,axis=0) for line in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Train acc 0.8643665027789588\n",
      "Train prec 0.8635565982483968\n",
      "Train rec 0.8643665027789588\n",
      "Train f1 0.8627645624408583\n",
      "Test acc 0.6949416342412451\n",
      "Test prec 0.7083793888096932\n",
      "Test rec 0.6949416342412451\n",
      "Test f1 0.6866830210018576\n",
      "{'C': 10, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "parameters =  {'penalty':('l1','l2'),'C': [0.1, 1, 10]}\n",
    "clf = LogisticRegression()\n",
    "estimator=busqueda_cv(clf,x_transformed,y_data,parameters)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(best.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Train acc 0.9190453815629187\n",
      "Train prec 0.9200924859323696\n",
      "Train rec 0.9190453815629187\n",
      "Train f1 0.9189740531517323\n",
      "Test acc 0.7042801556420234\n",
      "Test prec 0.7205827885876445\n",
      "Test rec 0.7042801556420234\n",
      "Test f1 0.699956429487653\n",
      "{'C': 10, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "  {'C': [0.1, 1, 10], 'kernel': ['linear']},\n",
    "  {'C': [0.1, 1, 10], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "clf = SVC()\n",
    "estimator=busqueda_cv(clf,x_transformed,y_data,parameters)\n",
    "best=compare_best_classifier(best,estimator)\n",
    "print(best.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
